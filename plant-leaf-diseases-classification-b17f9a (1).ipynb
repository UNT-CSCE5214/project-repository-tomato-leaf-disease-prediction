{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Import","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport cv2 as cv\nimport cv2 as cv2\nfrom matplotlib import pyplot as plt\nimport math\nimport os\nimport shutil\nfrom pathlib import Path\nimport glob\nimport itertools\nfrom itertools import cycle\nfrom scipy import interp\n\nfrom statistics import mean\nfrom statistics import stdev\n\n!pip install pyradiomics\nimport radiomics\nimport SimpleITK as sitk\nimport six\nfrom skimage.feature import greycomatrix, greycoprops\nfrom array import *\nfrom scipy.cluster.vq import whiten\nfrom scipy.cluster.vq import kmeans\n\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import label_binarize\nfrom sklearn.decomposition import PCA\n\nfrom sklearn import datasets\nfrom sklearn import svm\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.multiclass import OneVsRestClassifier\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_recall_fscore_support\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.metrics import average_precision_score\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import plot_precision_recall_curve\n\nimport matplotlib.pyplot as plt\nimport matplotlib.image as img","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-11-26T00:01:27.157707Z","iopub.execute_input":"2022-11-26T00:01:27.158330Z","iopub.status.idle":"2022-11-26T00:01:41.228211Z","shell.execute_reply.started":"2022-11-26T00:01:27.158253Z","shell.execute_reply":"2022-11-26T00:01:41.226959Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting pyradiomics\n  Downloading pyradiomics-3.0.1-cp37-cp37m-manylinux1_x86_64.whl (188 kB)\n\u001b[K     |████████████████████████████████| 188 kB 4.2 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: numpy>=1.9.2 in /opt/conda/lib/python3.7/site-packages (from pyradiomics) (1.19.5)\nRequirement already satisfied: PyWavelets>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from pyradiomics) (1.1.1)\nCollecting pykwalify>=1.6.0\n  Downloading pykwalify-1.8.0-py2.py3-none-any.whl (24 kB)\nRequirement already satisfied: six>=1.10.0 in /opt/conda/lib/python3.7/site-packages (from pyradiomics) (1.15.0)\nRequirement already satisfied: SimpleITK>=0.9.1 in /opt/conda/lib/python3.7/site-packages (from pyradiomics) (2.0.2)\nCollecting ruamel.yaml>=0.16.0\n  Downloading ruamel.yaml-0.17.21-py3-none-any.whl (109 kB)\n\u001b[K     |████████████████████████████████| 109 kB 47.7 MB/s eta 0:00:01\n\u001b[?25hCollecting docopt>=0.6.2\n  Downloading docopt-0.6.2.tar.gz (25 kB)\nRequirement already satisfied: python-dateutil>=2.8.0 in /opt/conda/lib/python3.7/site-packages (from pykwalify>=1.6.0->pyradiomics) (2.8.1)\nCollecting ruamel.yaml.clib>=0.2.6\n  Downloading ruamel.yaml.clib-0.2.7-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (500 kB)\n\u001b[K     |████████████████████████████████| 500 kB 75.5 MB/s eta 0:00:01\n\u001b[?25hBuilding wheels for collected packages: docopt\n  Building wheel for docopt (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13705 sha256=9754d9ea81ea8c32300944daa183808314b4f8ecc2945f9e61cb4a6728edb87a\n  Stored in directory: /root/.cache/pip/wheels/72/b0/3f/1d95f96ff986c7dfffe46ce2be4062f38ebd04b506c77c81b9\nSuccessfully built docopt\nInstalling collected packages: ruamel.yaml.clib, ruamel.yaml, docopt, pykwalify, pyradiomics\nSuccessfully installed docopt-0.6.2 pykwalify-1.8.0 pyradiomics-3.0.1 ruamel.yaml-0.17.21 ruamel.yaml.clib-0.2.7\n\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Folders and Files Paths","metadata":{}},{"cell_type":"code","source":"# Remember to include '/' at the end of the folder path only\n\n# Folder path for original images\noriBS = '../input/tomato-leaves-original-dataset/Dataset/Bacterial_Spot/'\noriEB = '../input/tomato-leaves-original-dataset/Dataset/Early_Blight/'\noriHL = '../input/tomato-leaves-original-dataset/Dataset/Healthy/'\noriLB = '../input/tomato-leaves-original-dataset/Dataset/Late_Blight/'\noriSLS = '../input/tomato-leaves-original-dataset/Dataset/Septorial_leaf_spot/'\noriYCV = '../input/tomato-leaves-original-dataset/Dataset/Yellow_Curl_Virus/'\n\n# Folder path for segmented images\nsegBS = './Bacterial_Spot_Segmented/'\nsegEB = './Early_Blight_Segmented/'\nsegHL = './Healthy_Segmented/'\nsegLB = './Late_Blight_Segmented/'\nsegSLS = './Septorial_Leaf_Spot_Segmented/'\nsegYCV = './Yellow_Curl_Virus_Segmented/'\n\n\n# File path for extracted features\nfeaBS = './Bacterial_Spot_Features.csv'\nfeaEB = './Early_Blight_Features.csv'\nfeaHL = './Healthy_Features.csv'\nfeaLB = './Late_Blight_Features.csv'\nfeaSLS = './Septorial_Leaf_Spot_Features.csv'\nfeaYCV = './Yellow_Curl_Virus_Features.csv'","metadata":{"execution":{"iopub.status.busy":"2022-11-26T00:01:41.230193Z","iopub.execute_input":"2022-11-26T00:01:41.230528Z","iopub.status.idle":"2022-11-26T00:01:41.239951Z","shell.execute_reply.started":"2022-11-26T00:01:41.230492Z","shell.execute_reply":"2022-11-26T00:01:41.236942Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Create the folders to store segmented images\n# If the folders had been created, do not run this cell\nos.makedirs(segBS)\nos.makedirs(segEB)\nos.makedirs(segHL)\nos.makedirs(segLB)\nos.makedirs(segSLS)\nos.makedirs(segYCV)","metadata":{"execution":{"iopub.status.busy":"2022-11-26T00:01:41.241968Z","iopub.execute_input":"2022-11-26T00:01:41.242275Z","iopub.status.idle":"2022-11-26T00:01:41.258320Z","shell.execute_reply.started":"2022-11-26T00:01:41.242248Z","shell.execute_reply":"2022-11-26T00:01:41.257339Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# ROI Segmentation and Image Pre-processing","metadata":{}},{"cell_type":"code","source":"\ndef segmentation(originalImageFolder, newFolderPath, cannyMin, cannyMax, numOfControusThreshold, lowerBoundDenominator, dilationIt, erodeIt, kernelDil, kernelEro):\n\n    fileList = []\n    fileIDList = []\n\n    fileIDList = next(os.walk(originalImageFolder), (None, None, []))[2] \n        \n    print('Number of Images: ', len(fileIDList))\n    i = 0\n    for file in fileIDList:\n        imgOriginal = cv.imread(originalImageFolder+file,1)\n        imgGray = cv.imread(originalImageFolder+file,0)\n        \n        \n        # Apply Gaussian Blur\n        imgGray = cv.GaussianBlur(imgGray,(3,3),0)\n\n        \n        # Apply Canny edge\n        edges = cv.Canny(imgGray,cannyMin,cannyMax)\n\n\n        # Apply filter contour by length\n        contours, hierarchy = cv2.findContours(edges,cv2.RETR_TREE,cv2.CHAIN_APPROX_NONE)\n        contours = sorted(contours, key=lambda x: cv2.arcLength(x, True))\n        if len(contours) > numOfControusThreshold:\n            imgContours = np.zeros(imgGray.shape, dtype=np.uint8)\n            lowerBound = max(numOfControusThreshold, int(len(contours)/lowerBoundDenominator))\n            for j in reversed(range(len(contours) - lowerBound,len(contours))):\n                length = cv2.arcLength(contours[j], True)\n                cv2.drawContours(imgContours, contours, j, (255), -1)\n        else:\n            imgContours = edges\n\n            \n        # Apply dilation\n        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (kernelDil,kernelDil))\n        dilation = cv2.dilate(imgContours,kernel,iterations = dilationIt)\n\n\n        # Find the max contours\n        contours, hierarchy = cv2.findContours(dilation,cv2.RETR_TREE,cv2.CHAIN_APPROX_NONE)\n        maxContour = max(contours, key = len)\n\n\n        # Draw it on image\n        img = np.zeros( imgGray.shape, dtype=np.uint8)\n        img = cv2.drawContours(img, [maxContour], -1, (255),-1)\n\n\n        # Erosion after filling contour\n        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (kernelEro,kernelEro))\n        img = cv2.erode(img,kernel,iterations = erodeIt)\n        segmented = cv2.bitwise_and(imgOriginal,imgOriginal,mask = img)\n\n        \n        # Convert from RGB to LAB colour space\n        lab = cv2.cvtColor(segmented, cv2.COLOR_BGR2LAB)\n        lab_planes = cv2.split(lab)\n        \n        \n        # Apply CLAHE\n        clahe = cv2.createCLAHE(clipLimit=5.0)\n        lab_planes[0] = clahe.apply(lab_planes[0])\n        lab = cv2.merge(lab_planes)\n\n        \n        # Convert back from RGB to LAB colour space\n        bgr = cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)\n\n        \n        # Write image into the folder\n        cv2.imwrite(newFolderPath + fileIDList[i], bgr)\n        i+=1\n        \n    # Create a zip file fir the folder to be downloaded\n    shutil.make_archive(newFolderPath, 'zip', newFolderPath)","metadata":{"execution":{"iopub.status.busy":"2022-11-26T00:01:41.259812Z","iopub.execute_input":"2022-11-26T00:01:41.260043Z","iopub.status.idle":"2022-11-26T00:01:41.274355Z","shell.execute_reply.started":"2022-11-26T00:01:41.260019Z","shell.execute_reply":"2022-11-26T00:01:41.273453Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"\n\ncannyMin = 40\ncannyMax = 125\nnumOfControusThreshold = 70\nlowerBoundDenominator = 2\ndilationIt = 20\nerodeIt = 15\nkernelDil = 3\nkernelEro = 3\nsegmentation(oriBS, segBS, cannyMin, cannyMax, numOfControusThreshold, \\\n             lowerBoundDenominator, dilationIt, erodeIt, kernelDil, kernelEro)\n\n\ncannyMin = 80\ncannyMax = 125\nnumOfControusThreshold = 50\nlowerBoundDenominator = 2\ndilationIt = 20\nerodeIt = 15\nsegmentation(oriEB, segEB, cannyMin, cannyMax, numOfControusThreshold, \\\n             lowerBoundDenominator, dilationIt, erodeIt, kernelDil, kernelEro)\n\n\ncannyMin = 80\ncannyMax = 125\nnumOfControusThreshold = 30\nlowerBoundDenominator = 2\ndilationIt = 20\nerodeIt = 15\n\nsegmentation(oriHL, segHL, cannyMin, cannyMax, numOfControusThreshold, \\\n             lowerBoundDenominator, dilationIt, erodeIt, kernelDil, kernelEro)\n\n\ncannyMin = 60\ncannyMax = 125\nnumOfControusThreshold = 70\nlowerBoundDenominator = 2\ndilationIt = 20\nerodeIt = 15\n\nsegmentation(oriLB, segLB, cannyMin, cannyMax, numOfControusThreshold, \\\n             lowerBoundDenominator, dilationIt, erodeIt, kernelDil, kernelEro)\n\n\ncannyMin = 100\ncannyMax = 125\nnumOfControusThreshold = 100\nlowerBoundDenominator = 4\ndilationIt = 30\nerodeIt = 20\n\nsegmentation(oriSLS, segSLS, cannyMin, cannyMax, numOfControusThreshold, \\\n             lowerBoundDenominator, dilationIt, erodeIt, kernelDil, kernelEro)\n\n\ncannyMin = 40\ncannyMax = 125\nnumOfControusThreshold = 40\nlowerBoundDenominator = 4\ndilationIt = 30\nerodeIt = 20\n\nsegmentation(oriYCV, segYCV, cannyMin, cannyMax, numOfControusThreshold, \\\n             lowerBoundDenominator, dilationIt, erodeIt, kernelDil, kernelEro)","metadata":{"execution":{"iopub.status.busy":"2022-11-26T00:01:41.275326Z","iopub.execute_input":"2022-11-26T00:01:41.275703Z","iopub.status.idle":"2022-11-26T00:02:42.853742Z","shell.execute_reply.started":"2022-11-26T00:01:41.275676Z","shell.execute_reply":"2022-11-26T00:02:42.852658Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Number of Images:  250\nNumber of Images:  250\nNumber of Images:  612\nNumber of Images:  250\nNumber of Images:  250\nNumber of Images:  250\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Feature Extraction","metadata":{}},{"cell_type":"markdown","source":"## Colour Histogram","metadata":{}},{"cell_type":"code","source":"\ndef ColourHistogram(folderPath,filename):\n    \n    # Read image\n    img = cv.imread(folderPath+filename)\n    \n    # Resize image\n    width = int(256)\n    height = int(256)\n    dim = (width, height)\n    img = cv.resize(img, dim, interpolation = cv.INTER_AREA)\n    \n    # Create df\n    x = [i for i in range(0,45)]\n    df = pd.DataFrame(columns = x)\n    \n    # Colour Histogram\n    Blue = cv.calcHist([img], [0], None, [256], [0,256])\n    Green = cv.calcHist([img], [1], None, [256], [0,256])\n    Red = cv.calcHist([img], [2], None, [256], [0,256])\n\n    result_R = [i for i in range(0,15)]\n    result_G = [i for i in range(0,15)]\n    result_B = [i for i in range(0,15)]\n\n    start = 0\n    end = 17\n    for i in range(0,15):\n        r = np.sum(Red[start:end])\n        g = np.sum(Green[start:end])\n        b = np.sum(Blue[start:end])\n        start = end\n        end = end + 17\n        result_R[i] = r\n        result_G[i] = g\n        result_B[i] = b\n\n    finalResult = [filename]\n    finalResult = result_R + result_G + result_B + finalResult\n\n    df = df.append([finalResult])\n    df['ID'] = df[45]\n    df.drop(45, axis = 1, inplace = True)\n    df = df.set_index('ID')\n\n    return df","metadata":{"execution":{"iopub.status.busy":"2022-11-26T00:02:42.855788Z","iopub.execute_input":"2022-11-26T00:02:42.856225Z","iopub.status.idle":"2022-11-26T00:02:42.869697Z","shell.execute_reply.started":"2022-11-26T00:02:42.856171Z","shell.execute_reply":"2022-11-26T00:02:42.867641Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"## Dominant Colours","metadata":{}},{"cell_type":"code","source":"\ndef DominantColour(folderPath,filename):\n    \n    # Read image\n    img = cv.imread(folderPath+filename)\n    \n    \n    # Resize image\n    width = int(256)\n    height = int(256)\n    dim = (width, height)\n    img = cv.resize(img, dim, interpolation = cv.INTER_AREA)\n    \n    \n    # Create df\n    df = pd.DataFrame()\n    \n    \n    # RGB\n    r = []\n    g = []\n    b = []\n    for row in img:\n        for temp_r, temp_g, temp_b in row:\n            r.append(temp_r)\n            g.append(temp_g)\n            b.append(temp_b)\n            \n    dfKMeans = pd.DataFrame({'red' : r,\n                          'green' : g,\n                          'blue' : b})\n \n    dfKMeans['scaled_color_red'] = whiten(dfKMeans['red'])\n    dfKMeans['scaled_color_blue'] = whiten(dfKMeans['blue'])\n    dfKMeans['scaled_color_green'] = whiten(dfKMeans['green'])\n\n    cluster_centers, _ = kmeans(dfKMeans[['scaled_color_red',\n                                    'scaled_color_blue',\n                                    'scaled_color_green']], 3)\n\n    dominant_colors = [filename]\n\n    red_std, green_std, blue_std = dfKMeans[['red','green','blue']].std()\n\n    values = [filename]\n    for i, cluster_center in enumerate(cluster_centers):\n        red_scaled, green_scaled, blue_scaled = cluster_center\n        values.append(red_scaled * red_std / 255)\n        values.append(green_scaled * green_std / 255)\n        values.append(blue_scaled * blue_std / 255)\n\n    while len(values) < 10:\n        values.append(pd.NA)\n        \n    df = df.append([values])\n    df.columns = ['ID','Red1','Green1','Blue1','Red2','Green2','Blue2','Red3','Green3','Blue3']\n    df = df.set_index('ID')\n\n    return df","metadata":{"execution":{"iopub.status.busy":"2022-11-26T00:02:42.871714Z","iopub.execute_input":"2022-11-26T00:02:42.872103Z","iopub.status.idle":"2022-11-26T00:02:42.898368Z","shell.execute_reply.started":"2022-11-26T00:02:42.872076Z","shell.execute_reply":"2022-11-26T00:02:42.897147Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"## GLCM","metadata":{}},{"cell_type":"code","source":"\ndef GLCM(folderPath,filename):\n    # Read image\n    img = cv.imread(folderPath+filename, 0)\n    \n    # Resize image\n    width = int(256)\n    height = int(256)\n    dim = (width, height)\n    img = cv.resize(img, dim, interpolation = cv.INTER_AREA)\n    \n    df = pd.DataFrame()\n    df['ID'] = [filename]\n\n    #Full image\n    #GLCM = greycomatrix(img, [1], [0, np.pi/4, np.pi/2, 3*np.pi/4])\n    GLCM = greycomatrix(img, [1], [0])       \n    GLCM_Energy = greycoprops(GLCM, 'energy')[0]\n    df['Energy'] = GLCM_Energy\n    GLCM_corr = greycoprops(GLCM, 'correlation')[0]\n    df['Corr'] = GLCM_corr       \n    GLCM_diss = greycoprops(GLCM, 'dissimilarity')[0]\n    df['Diss_sim'] = GLCM_diss       \n    GLCM_hom = greycoprops(GLCM, 'homogeneity')[0]\n    df['Homogen'] = GLCM_hom       \n    GLCM_contr = greycoprops(GLCM, 'contrast')[0]\n    df['Contrast'] = GLCM_contr\n\n    GLCM2 = greycomatrix(img, [3], [0])       \n    GLCM_Energy2 = greycoprops(GLCM2, 'energy')[0]\n    df['Energy2'] = GLCM_Energy2\n    GLCM_corr2 = greycoprops(GLCM2, 'correlation')[0]\n    df['Corr2'] = GLCM_corr2       \n    GLCM_diss2 = greycoprops(GLCM2, 'dissimilarity')[0]\n    df['Diss_sim2'] = GLCM_diss2       \n    GLCM_hom2 = greycoprops(GLCM2, 'homogeneity')[0]\n    df['Homogen2'] = GLCM_hom2       \n    GLCM_contr2 = greycoprops(GLCM2, 'contrast')[0]\n    df['Contrast2'] = GLCM_contr2\n\n    GLCM3 = greycomatrix(img, [5], [0])       \n    GLCM_Energy3 = greycoprops(GLCM3, 'energy')[0]\n    df['Energy3'] = GLCM_Energy3\n    GLCM_corr3 = greycoprops(GLCM3, 'correlation')[0]\n    df['Corr3'] = GLCM_corr3       \n    GLCM_diss3 = greycoprops(GLCM3, 'dissimilarity')[0]\n    df['Diss_sim3'] = GLCM_diss3       \n    GLCM_hom3 = greycoprops(GLCM3, 'homogeneity')[0]\n    df['Homogen3'] = GLCM_hom3       \n    GLCM_contr3 = greycoprops(GLCM3, 'contrast')[0]\n    df['Contrast3'] = GLCM_contr3\n\n    GLCM4 = greycomatrix(img, [0], [np.pi/4])       \n    GLCM_Energy4 = greycoprops(GLCM4, 'energy')[0]\n    df['Energy4'] = GLCM_Energy4\n    GLCM_corr4 = greycoprops(GLCM4, 'correlation')[0]\n    df['Corr4'] = GLCM_corr4       \n    GLCM_diss4 = greycoprops(GLCM4, 'dissimilarity')[0]\n    df['Diss_sim4'] = GLCM_diss4       \n    GLCM_hom4 = greycoprops(GLCM4, 'homogeneity')[0]\n    df['Homogen4'] = GLCM_hom4       \n    GLCM_contr4 = greycoprops(GLCM4, 'contrast')[0]\n    df['Contrast4'] = GLCM_contr4\n\n    GLCM5 = greycomatrix(img, [0], [np.pi/2])       \n    GLCM_Energy5 = greycoprops(GLCM5, 'energy')[0]\n    df['Energy5'] = GLCM_Energy5\n    GLCM_corr5 = greycoprops(GLCM5, 'correlation')[0]\n    df['Corr5'] = GLCM_corr5       \n    GLCM_diss5 = greycoprops(GLCM5, 'dissimilarity')[0]\n    df['Diss_sim5'] = GLCM_diss5       \n    GLCM_hom5 = greycoprops(GLCM5, 'homogeneity')[0]\n    df['Homogen5'] = GLCM_hom5       \n    GLCM_contr5 = greycoprops(GLCM5, 'contrast')[0]\n    df['Contrast5'] = GLCM_contr5\n\n    GLCM6 = greycomatrix(img, [0], [3*np.pi/4])       \n    GLCM_Energy6 = greycoprops(GLCM6, 'energy')[0]\n    df['Energy6'] = GLCM_Energy6\n    GLCM_corr6 = greycoprops(GLCM6, 'correlation')[0]\n    df['Corr6'] = GLCM_corr6       \n    GLCM_diss6 = greycoprops(GLCM6, 'dissimilarity')[0]\n    df['Diss_sim6'] = GLCM_diss6       \n    GLCM_hom6 = greycoprops(GLCM6, 'homogeneity')[0]\n    df['Homogen6'] = GLCM_hom6       \n    GLCM_contr6 = greycoprops(GLCM6, 'contrast')[0]\n    df['Contrast6'] = GLCM_contr6\n\n    df = df.set_index('ID')\n    return df","metadata":{"execution":{"iopub.status.busy":"2022-11-26T00:02:42.900069Z","iopub.execute_input":"2022-11-26T00:02:42.900350Z","iopub.status.idle":"2022-11-26T00:02:42.920513Z","shell.execute_reply.started":"2022-11-26T00:02:42.900298Z","shell.execute_reply":"2022-11-26T00:02:42.918621Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"## GLSZM","metadata":{}},{"cell_type":"code","source":"\ndef GLSZM(folderPath, filename):\n    # Read image\n    #img = cv.imread(filename)\n    img = cv.imread(folderPath+filename, 0)\n    # Resize image\n    width = int(256)\n    height = int(256)\n    dim = (width, height)\n    img = cv.resize(img, dim, interpolation = cv.INTER_AREA)\n    \n    height = img.shape[0]\n    width = img.shape[1]\n    tempMask = np.ones((height,width), np.uint8)\n\n    img = sitk.GetImageFromArray(img)\n    mask = sitk.GetImageFromArray(tempMask)\n    \n    # Create df\n    df = pd.DataFrame()\n    \n    # Generate Features\n    glszmFeatures = radiomics.glszm.RadiomicsGLSZM(img, mask)\n    glszmFeatures.enableAllFeatures()\n    result = glszmFeatures.execute()\n    \n    values = [filename]\n    for (key, val) in six.iteritems(result):\n        values.append(val.item())\n\n    # Append the value into the df and define the column names\n    df = df.append([values])\n    df.columns = ['ID', 'GrayLevelNonUniformity', 'GrayLevelNonUniformityNormalized',\n       'GrayLevelVariance', 'HighGrayLevelZoneEmphasis', 'LargeAreaEmphasis',\n       'LargeAreaHighGrayLevelEmphasis', 'LargeAreaLowGrayLevelEmphasis',\n       'LowGrayLevelZoneEmphasis', 'SizeZoneNonUniformity',\n       'SizeZoneNonUniformityNormalized', 'SmallAreaEmphasis',\n       'SmallAreaHighGrayLevelEmphasis', 'SmallAreaLowGrayLevelEmphasis',\n       'ZoneEntropy', 'ZonePercentage', 'ZoneVariance']\n    df = df.set_index('ID')\n    return df","metadata":{"execution":{"iopub.status.busy":"2022-11-26T00:02:42.923344Z","iopub.execute_input":"2022-11-26T00:02:42.923716Z","iopub.status.idle":"2022-11-26T00:02:42.937635Z","shell.execute_reply.started":"2022-11-26T00:02:42.923689Z","shell.execute_reply":"2022-11-26T00:02:42.936621Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"## Extract All Features","metadata":{}},{"cell_type":"code","source":"\ndef FeaturesExtractionsAll(folderPath):\n    df = pd.DataFrame()\n    for fname in sorted(os.listdir(folderPath)):\n        colourHist = ColourHistogram(folderPath,fname)\n        dominantCol = DominantColour(folderPath,fname)\n        glcm = GLCM(folderPath,fname)\n        glszm = GLSZM(folderPath,fname)\n        dfRow = colourHist.join([dominantCol, glcm, glszm])\n        df = df.append(dfRow)\n            \n    return df","metadata":{"execution":{"iopub.status.busy":"2022-11-26T00:02:42.939039Z","iopub.execute_input":"2022-11-26T00:02:42.939317Z","iopub.status.idle":"2022-11-26T00:02:42.959635Z","shell.execute_reply.started":"2022-11-26T00:02:42.939291Z","shell.execute_reply":"2022-11-26T00:02:42.958358Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"\ndfBacterialSpot = FeaturesExtractionsAll(segBS)\ndfBacterialSpot.to_csv(feaBS)\n\ndfEarlyBlight = FeaturesExtractionsAll(segEB)\ndfEarlyBlight.to_csv(feaEB)\n\ndfHealthy = FeaturesExtractionsAll(segHL)\ndfHealthy.to_csv(feaHL)\n\ndfLateBlight = FeaturesExtractionsAll(segLB)\ndfLateBlight.to_csv(feaLB)\n\ndfSeptorial = FeaturesExtractionsAll(segSLS)\ndfSeptorial.to_csv(feaSLS)\n\ndfYellowCurl = FeaturesExtractionsAll(segYCV)\ndfYellowCurl.to_csv(feaYCV)","metadata":{"execution":{"iopub.status.busy":"2022-11-26T00:02:42.960923Z","iopub.execute_input":"2022-11-26T00:02:42.961166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"markdown","source":"## Combine All Classes' Features and Remove Outliers","metadata":{}},{"cell_type":"code","source":"\ndef CombineAllDF(folderPaths, labels):\n    dfAll = pd.DataFrame()\n    i=0\n    for paths, label in zip(folderPaths, labels):\n        dfs = pd.read_csv(paths)\n        \n        dfs['Labels'] = labels[i]\n        dfs = dfs.set_index('ID')\n        \n        dfAll = dfAll.append(dfs)\n        i=i+1\n    dfAll = dfAll.reset_index()\n    return dfAll","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\n# Combine all df from each class into one big df\n# So the final df contains a df with all classess, and all features\n\nbacterialSpot = feaBS\nearlyBlight = feaEB\nhealthy = feaHL\nlateBlight = feaLB\nseptorialSpot = feaSLS\nyellowCurl = feaYCV\n\n\nfolderPaths = [bacterialSpot,earlyBlight,healthy,lateBlight,septorialSpot,yellowCurl]\nclasses = ['Bacterial Spot','Early Blight','Healthy','Late Blight','Septorial Leaf Spot','Yellow Curl Virus']\ndfALL = CombineAllDF(folderPaths, classes)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n    \n# Drop ID and LABELS\n# Because this is to remove outliers only\ncols = dfALL.drop(['ID','Labels'], axis = 1).columns\n\n# Remove outliers\nQ1 = dfALL[cols].quantile(0.25)\nQ3 = dfALL[cols].quantile(0.75)\nIQR = Q3 - Q1\n\ndfFinal = dfALL[~((dfALL[cols] < (Q1 - 1.5 * IQR)) | (dfALL[cols] > (Q3 + 1.5 * IQR))).any(axis=1)]\ndfFinal = dfFinal.reset_index()\ndfFinal.drop('index',inplace = True, axis = 1)\ndfFinal['Red3'] = dfFinal['Red3'].replace(np.nan, 0.277547)\ndfFinal['Green3'] = dfFinal['Green3'].replace(np.nan, 0.34919)\ndfFinal['Blue3'] = dfFinal['Blue3'].replace(np.nan, 0.340418)\nprint(dfFinal.shape)\n\n# Generate X_train, without ID and Labels\n# Y_train only contains labels\nX = dfFinal.drop(['ID','Labels'], axis = 1)\ny = dfFinal.loc[:,'Labels']\n\n# split the dataset into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42)\nprint('Training set shape: ', X_train.shape, y_train.shape)\nprint('Testing set shape: ', X_test.shape, y_test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n    \n# List out all files that have been removed\ndfOutliers = dfALL[((dfALL[cols] < (Q1 - 1.5 * IQR)) | (dfALL[cols] > (Q3 + 1.5 * IQR))).any(axis=1)]\nfor i in range(dfOutliers.shape[0]):\n    print(dfOutliers['ID'].iloc[i])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## K-Fold Cross Validation","metadata":{}},{"cell_type":"code","source":"\n\n# Use 10-fold cross validation\nn_splits = 10\n\n# Number of components/features after PCA\nn_components = int(len(cols)/5)\n\nkf = KFold(n_splits=n_splits, random_state=42, shuffle=True)\n\n\n# Create a list to store all metrics for training and evaluating\naccuracies_tr = []\nprecisions_tr = []\nrecalls_tr = []\nf1s_tr = []\n\naccuracies_val = []\nprecisions_val = []\nrecalls_val = []\nf1s_val = []\n\n\nfor i, (train_index, test_index) in enumerate(kf.split(X_train)):\n    # split the training set into train and val sets\n    X_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train, random_state = 42)\n\n    \n    # Mean Imputation\n    imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n    imp.fit(X_tr)\n    imp.transform(X_tr)\n    imp.transform(X_val)\n    \n    \n    # Standard Scaler\n    scaler = StandardScaler()\n    scaler.fit(X_tr)\n    X_tr = scaler.transform(X_tr)\n    X_val = scaler.transform(X_val)\n    \n    \n    # PCA\n    pca = PCA(n_components = n_components)\n    pca.fit(X_tr)\n    X_tr = pca.transform(X_tr)\n    X_val = pca.transform(X_val)\n    \n    \n    # Classifier\n    #clf =  #svm.SVC()\n    clf = DecisionTreeClassifier(random_state = 42)\n    clf.fit(X_tr, y_tr)\n    y_pred_tr = clf.predict(X_tr)\n    y_pred_val = clf.predict(X_val)\n    \n    \n    # Print current split count\n    print('Current Split: ', i)\n    i+=1\n    \n    \n    # Evaluate Train\n    accuracy = accuracy_score(y_tr, y_pred_tr)\n    pre, rec, f1, _ = precision_recall_fscore_support(y_tr, y_pred_tr, average='macro', zero_division = 0)\n    print('Training')\n    print('Accuracy: ',accuracy)\n    print('Precision: ', pre)\n    print('Recall: ', rec)\n    print('F1: ', f1)\n    con_tr = confusion_matrix(y_tr, y_pred_tr, labels=classes)#Labels\n\n    \n    accuracies_tr.append(accuracy)\n    precisions_tr.append(pre)\n    recalls_tr.append(rec)\n    f1s_tr.append(f1)\n    \n    \n    # Evaluate Val\n    accuracy = accuracy_score(y_val, y_pred_val)\n    pre, rec, f1, _ = precision_recall_fscore_support(y_val, y_pred_val, average='macro', zero_division = 0)\n    print('\\n')\n    print('Validation')\n    print('Accuracy: ',accuracy)\n    print('Precision: ', pre)\n    print('Recall: ', rec)\n    print('F1: ', f1)\n    con_val = confusion_matrix(y_val, y_pred_val, labels=classes)#Labels\n\n    \n    accuracies_val.append(accuracy)\n    precisions_val.append(pre)\n    recalls_val.append(rec)\n    f1s_val.append(f1)\n    \n    \n    print('\\n')\n    print('\\n')\n    print('\\n')\n    \nprint('Average training accuracy: ', mean(accuracies_tr))\nprint('Average training precision: ', mean(precisions_tr))\nprint('Average training recall: ', mean(recalls_tr))\nprint('Average training f1: ', mean(f1s_tr))\nprint('\\n')\nprint('Average validation accuracy: ', mean(accuracies_val))\nprint('Average validation precision: ', mean(precisions_val))\nprint('Average validation recall: ', mean(recalls_val))\nprint('Average validation f1: ', mean(f1s_val))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Hyperparameter Tuning","metadata":{}},{"cell_type":"code","source":"\n\n# List out parameters to perform grid search\nparameters = {'kernel':['linear', 'poly','rbf', 'sigmoid'], \n      'C':[0.1, 0.5, 1, 5, 10],\n      'degree':[1, 3, 5, 7, 9],\n      'class_weight':[None, 'balanced']\n     }\n\n# This one list all the scoring that we wants\nscoring = ['accuracy','precision_macro','recall_macro','f1_macro']\n\n# Transform input data\nscaler = StandardScaler()\nscaler.fit(X_train)\nX_tr = scaler.transform(X_train)\n\npca = PCA(n_components = n_components)\npca.fit(X_tr)\nX_tr = pca.transform(X_tr)\n\n# Start hyperparameter tuning\nclf = svm.SVC(decision_function_shape = 'ovr')\nclf = GridSearchCV(clf, parameters, cv = 5, scoring = scoring, refit = 'accuracy')\n\n# Fit the model on our train set\nclf.fit(X_tr, y_train)\n\n# Find the best score\nprint(clf.best_score_)\n\n# Get the hyperparameters with the best score\nprint(clf.best_params_)\nbest_clf = clf.best_params_\n\n# Record the best estimator\nprint(clf.best_estimator_)\n\n\n### THERE MIGHT BE WARNING FOR PRECISION ILL-DEFINED DUE TO DIVISION BY 0","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}